{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0539f9df",
   "metadata": {},
   "source": [
    "# Scraping Top 30 Repositories for Topics on GitHub\n",
    "\n",
    "## Introduction\n",
    "\n",
    "### Web Scraping : \n",
    "- Web scraping is a technique used to extract data from websites. It involves fetching the web page's HTML code and then parsing  it to extract the desired information. This can be done manually, but it is more commonly automated using programming scripts and specialized tools.\n",
    "\n",
    "### GitHub : \n",
    "- GitHub is a web-based platform that provides a hosting service for version control using Git. It is widely used for source code management and collaboration in software development projects.\n",
    "\n",
    "### Problem Statement : \n",
    "- We have to scrape top repositories for topics on GitHUb. \n",
    "- Andconvert it them into the csv format.\n",
    "- So, further we can use them for data anlysis , For Researches etc...\n",
    " \n",
    "### Tools : \n",
    "- Python \n",
    "- Jupyter Notebook\n",
    "- Requests\n",
    "- Beautiful Soup \n",
    "- Pandas  \n",
    "- os\n",
    "- Developer's Tool of a Browser\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcea9894",
   "metadata": {},
   "source": [
    "Here are the steps we'll follow:\n",
    "\n",
    "- We're going to scrape https://github.com/topics\n",
    "- We'll get a list of topics. For each topic, we'll get topic title, topic page URL and topic description\n",
    "- For each topic, we'll get the top 30 repositories in the topic from the topic page\n",
    "- For each repository, we'll grab the repo name, username, stars and repo URL\n",
    "- For each topic we'll create a CSV file in the following format:\n",
    "\n",
    "```\n",
    "Repo Name,Username,Stars,Repo URL\n",
    "three.js,mrdoob,69700,https://github.com/mrdoob/three.js\n",
    "libgdx,libgdx,18300,https://github.com/libgdx/libgdx\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72400e83",
   "metadata": {},
   "source": [
    "## Step : 1 :- Scrape the list of topics from Github\n",
    "\n",
    "- First of all we need to install all the require libraris that we are going to use in this Project. \n",
    "- After the import them accordingly.\n",
    "- Now , import and use requests library to download the page('s html code). \n",
    "- After downloading the page into Jupyter use BS4 to parse that html code and extract information from it.\n",
    "- And then after extracting all the information using BS4 use pandas library to covert it to a DataFrame.\n",
    "- convert to a Pandas dataframe\n",
    "\n",
    "Let's write a function to download the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ae1ab54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_topics_page():\n",
    "    # TODO - add comments\n",
    "    topics_url = 'https://github.com/topics'\n",
    "    response = requests.get(topics_url)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception('Failed to load page {}'.format(topic_url))\n",
    "    doc = BeautifulSoup(response.text, 'html.parser')\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed52a07",
   "metadata": {},
   "source": [
    "- `get_topics_page` takes the url of the webpage that we have to scrape and returns the response object that containing that web page's html code.\n",
    "- After getting response it is checking for the sucessful response.\n",
    "- For Sucessful response the code is inbetween (200-299) inclusively.\n",
    "- And when the page does't downloaded then it raise an Exception.\n",
    "- If response is sucessful then it's parse the text format of that response using .text method and using html.parser.\n",
    "- And returns a Beatiful Soup Document.\n",
    "\n",
    "And below one xample of this function is also obtained to check how it's working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f23b1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = get_topics_page()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3840049",
   "metadata": {},
   "source": [
    "### When a BS4 document is created then , \n",
    "- We have to fetch the informations that we are looking for using Web Scaraping.  \n",
    "- Now , We have to go to the website which we want to scrape.\n",
    "- Inspect that website using inspect element and Devloper's Tools.\n",
    "- Start getting all the require data according to our project.\n",
    "- Using Beatiful Soup's find() ,find_all() etc methods.\n",
    "\n",
    "Let's create some helper functions to parse information from the page.\n",
    "\n",
    "To get topic titles, we can pick `p` tags with the `class` ...\n",
    "\n",
    "![](https://i.imgur.com/OnzIdyP.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad39619f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_titles(doc):\n",
    "    selection_class = 'f3 lh-condensed mb-0 mt-1 Link--primary'\n",
    "    topic_title_tags = doc.find_all('p', {'class': selection_class})\n",
    "    topic_titles = []\n",
    "    for tag in topic_title_tags:\n",
    "        topic_titles.append(tag.text)\n",
    "    return topic_titles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f2f6f1",
   "metadata": {},
   "source": [
    "- `get_topic_titles` can be used to get the list of titles.\n",
    "- Using BS4 we are targeting the tags and the specific classes or id which are representing the titles.\n",
    "- Using .text method we can fetch them out from their html element.\n",
    "- For loop is used to get all titles present on current page.\n",
    "- And append them int a list named `topic_titles` and returns it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9baaf1ab",
   "metadata": {},
   "source": [
    "Below one example is done to demonstrate the function\n",
    "- There are 30 title on current page.\n",
    "- List of the 30 titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20d363af",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = get_topic_titles(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00f6287a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b020b282",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3D', 'Ajax', 'Algorithm', 'Amp', 'Android']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a459d1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "212ddd39",
   "metadata": {},
   "source": [
    "Similarly we have defined functions for descriptions and URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6bbb8dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_descs(doc):\n",
    "    desc_selector = 'f5 color-fg-muted mb-0 mt-1'\n",
    "    topic_desc_tags = doc.find_all('p', {'class': desc_selector})\n",
    "    topic_descs = []\n",
    "    for tag in topic_desc_tags:\n",
    "        topic_descs.append(tag.text.strip())\n",
    "    return topic_descs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfeaaef2",
   "metadata": {},
   "source": [
    "- `get_topic_descs` can be used to get the list of description. \n",
    "- Takes BS4 document and using BS4.\n",
    "- Tageting the tags and the specific classes or id which are representing the descriptions.\n",
    "- Using for loop it gets appended in the list called ` topic_descs`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cb1a70",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fafccc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_urls(doc):\n",
    "    topic_link_tags = doc.find_all('a', {'class': 'no-underline flex-1 d-flex flex-column'})\n",
    "    topic_urls = []\n",
    "    base_url = 'https://github.com'\n",
    "    for tag in topic_link_tags:\n",
    "        topic_urls.append(base_url + tag['href'])\n",
    "    return topic_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86110ba",
   "metadata": {},
   "source": [
    "- `get_topic_urls` can be used to get the list of urls.\n",
    "- Here w need a `base_url` which is concatinated with the topics's url.\n",
    "- For loop get us list of all urls on the current page."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d435ba",
   "metadata": {},
   "source": [
    "Let's put this all together into a single function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e36746bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_topics():\n",
    "    topics_url = 'https://github.com/topics'\n",
    "    response = requests.get(topics_url)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception('Failed to load page {}'.format(topic_url))\n",
    "    doc = BeautifulSoup(response.text, 'html.parser')\n",
    "    topics_dict = {\n",
    "        'title': get_topic_titles(doc),\n",
    "        'description': get_topic_descs(doc),\n",
    "        'url': get_topic_urls(doc)\n",
    "    }\n",
    "    return pd.DataFrame(topics_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d8f02c",
   "metadata": {},
   "source": [
    "- `scrape_topics` topic is a nested function which combines all the above functions.\n",
    "- And creating the dictionary of topics.\n",
    "- Using pandas converting it into a DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afec039e",
   "metadata": {},
   "source": [
    "## Step : 2 :- Get the top 30 repositories from a topic page\n",
    "\n",
    "- We have topic's title , description and urls.\n",
    "- Now , further using them we have to scrape top 30 repositories from each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a473cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_page(topic_url):\n",
    "    # Download the page\n",
    "    response = requests.get(topic_url)\n",
    "    # Check successful response\n",
    "    if response.status_code != 200:\n",
    "        raise Exception('Failed to load page {}'.format(topic_url))\n",
    "    # Parse using Beautiful soup\n",
    "    topic_doc = BeautifulSoup(response.text, 'html.parser')\n",
    "    return topic_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b217712",
   "metadata": {},
   "source": [
    "- `get_topic_page` takes topic url and using requests it download that page.\n",
    "- Check the response.\n",
    "- Create a BS4 document.\n",
    "\n",
    "Below one example for one topic is given"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c89935cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = get_topic_page('https://github.com/topics/3d')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d5460b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a41b1253",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_star_count(stars):\n",
    "    stars=stars.strip()\n",
    "    if stars[-1]=='k':\n",
    "        return int(float(stars[:-1])*1000)\n",
    "    return(int(stars))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d24b3d1",
   "metadata": {},
   "source": [
    "- `parse_star_count` coverts strings like '96.7k' into integer like 96700.\n",
    "- It takes string as an argument and return integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01b32d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f7638a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_repo_info(h1_tag, star_tag):\n",
    "    # returns all the required info about a repository\n",
    "    a_tags = h1_tag.find_all('a')\n",
    "    username = a_tags[0].text.strip()\n",
    "    repo_name = a_tags[1].text.strip()\n",
    "    repo_url =  base_url + a_tags[1]['href']\n",
    "    stars = parse_star_count(star_tag.text.strip())\n",
    "    return username, repo_name, stars, repo_url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e04e15",
   "metadata": {},
   "source": [
    "-`get_repo_info` takes varriable which has article tag.\n",
    "- Which contains `repo name` and `user name`.\n",
    "- The span tag is child of article tag.\n",
    "- And contains Repositories star count.\n",
    "- And the function returns all the details related to the repositories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936d600c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a42e6157",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_repos(topic_doc):\n",
    "    # Get the h1 tags containing repo title, repo URL and username\n",
    "    repo_tags = topic_doc.find_all('article',{'class':'border rounded color-shadow-small color-bg-subtle my-4'})\n",
    "\n",
    "    # Get star tags\n",
    "    star_tags=topic_doc.find_all('span',{'id':'repo-stars-counter-star'})\n",
    "    \n",
    "    topic_repos_dict = { 'username': [], 'repo_name': [], 'stars': [],'repo_url': []}\n",
    "\n",
    "    # Get repo info\n",
    "    for i in range(len(repo_tags)):\n",
    "        repo_info = get_repo_info(repo_tags[i], star_tags[i])\n",
    "        topic_repos_dict['username'].append(repo_info[0])\n",
    "        topic_repos_dict['repo_name'].append(repo_info[1])\n",
    "        topic_repos_dict['stars'].append(repo_info[2])\n",
    "        topic_repos_dict['repo_url'].append(repo_info[3])\n",
    "        \n",
    "    return pd.DataFrame(topic_repos_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94eba4f2",
   "metadata": {},
   "source": [
    "- `get_topic_repos` can be used to get the list of repo name , user name and stars.\n",
    "- Tageting the tags and the specific classes or id which are representing the repo name , user name and stars.\n",
    "- Put all the lists of details in dictionary named `topic_repos_name`.\n",
    "- And convert that dictionary into a DataFrame using pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f9c990",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3652ef0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_topic(topic_url, path):\n",
    "    if os.path.exists(path):\n",
    "        print(\"The file {} already exists. Skipping...\".format(path))\n",
    "        return\n",
    "    topic_df = get_topic_repos(get_topic_page(topic_url))\n",
    "    topic_df.to_csv(path, index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380143f7",
   "metadata": {},
   "source": [
    "- `scrape_topic` takes topic url and path which it can be located.\n",
    "- OS module checks if the path of the file is existing or not.\n",
    "- If , Path exists it skips that file and move on to the next file.\n",
    "- If , Path does't exists then it creates a DataFrame then converts it into a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4edbb7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "037219ac",
   "metadata": {},
   "source": [
    "## Putting it all together\n",
    "\n",
    "- We have a funciton to get the list of topics\n",
    "- We have a function to create a CSV file for scraped repos from a topics page\n",
    "- Let's create a function to put them together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f279e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "base_url=\"https://github.com\"\n",
    "def scrape_topics_repos():\n",
    "    print('Scraping list of topics')\n",
    "    topics_df = scrape_topics()\n",
    "    \n",
    "    os.makedirs('data', exist_ok=True)\n",
    "    for index, row in topics_df.iterrows():\n",
    "        print('Scraping top repositories for \"{}\"'.format(row['title']))\n",
    "        scrape_topic(row['url'], 'data/{}.csv'.format(row['title']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14206e53",
   "metadata": {},
   "source": [
    "Let's run it to scrape the top repos for the all the topics on the first page of https://github.com/topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "12cf36ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping list of topics\n",
      "Scraping top repositories for \"3D\"\n",
      "Scraping top repositories for \"Ajax\"\n",
      "Scraping top repositories for \"Algorithm\"\n",
      "Scraping top repositories for \"Amp\"\n",
      "Scraping top repositories for \"Android\"\n",
      "Scraping top repositories for \"Angular\"\n",
      "Scraping top repositories for \"Ansible\"\n",
      "Scraping top repositories for \"API\"\n",
      "Scraping top repositories for \"Arduino\"\n",
      "Scraping top repositories for \"ASP.NET\"\n",
      "Scraping top repositories for \"Atom\"\n",
      "Scraping top repositories for \"Awesome Lists\"\n",
      "Scraping top repositories for \"Amazon Web Services\"\n",
      "Scraping top repositories for \"Azure\"\n",
      "Scraping top repositories for \"Babel\"\n",
      "Scraping top repositories for \"Bash\"\n",
      "Scraping top repositories for \"Bitcoin\"\n",
      "Scraping top repositories for \"Bootstrap\"\n",
      "Scraping top repositories for \"Bot\"\n",
      "Scraping top repositories for \"C\"\n",
      "Scraping top repositories for \"Chrome\"\n",
      "Scraping top repositories for \"Chrome extension\"\n",
      "Scraping top repositories for \"Command line interface\"\n",
      "Scraping top repositories for \"Clojure\"\n",
      "Scraping top repositories for \"Code quality\"\n",
      "Scraping top repositories for \"Code review\"\n",
      "Scraping top repositories for \"Compiler\"\n",
      "Scraping top repositories for \"Continuous integration\"\n",
      "Scraping top repositories for \"COVID-19\"\n",
      "Scraping top repositories for \"C++\"\n"
     ]
    }
   ],
   "source": [
    "scrape_topics_repos()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
